##### Task one

Data description:
It was generated by exporting data from a SQL database.
The data in at least one of the fields (the body field) can include newline characters, 
and all the fields are enclosed in double quotes. 
Each 'line' will be a list that contains each field in sequential order.

Upzip dataset
$ tar zxvf forum_data.tar.gz

Look at data
$ head forum_node.tsv
$ head forum_users.tsv

Format of each line in forum_node.tsv is: 
"id" "title" "tagnames"	"author_id" "body" "node_type" "parent_id" "abs_parent_id" "added_at"
"score"	"state_string"	"last_edited_id" "last_activity_by_id" "last_activity_at" 
"active_revision_id"	"extra"	"extra_ref_id"	"extra_count"	"marked"

The above is the first line. Not every item has value. It may be "" or "\N"
"5339"	"Whether pdf of Unit and Homework is available?" "cs101 pdf" "100000458" ""	
"question"	"\N"	"\N"	"2012-02-25 08:09:06.787181+00"	"1"	"" "\N"	"100000921"	
"2012-02-25 08:11:01.623548+00"	"6922"	"\N"	"\N"	"204"	"f"

Format of each line in forum_users.tsv is: 
"user_ptr_id"	"reputation"	"gold"	"silver"	"bronze"

Write a MapReduce program that creates an index of all words that can be found 
in the body of a forum post and node id they can be found in
Do not parse the HTML. Just split the text on all whitespace as well as the following
characters: .,!?:;"()<>[]#$=-/

Test MapReduce scripts
$ head -50 forum_node.tsv | ./mapper1.py | sort | ./reducer1.py > results.txt

Run Hadoop job
$ hadoop fs -put forum_node.tsv myinput/
$ hs mapper1.py reducer1.py myinput/forum_node.tsv l4_output1

Q1: How many times was the word 'fantastic' used on forums?
$ hadoop fs -cat l4_output1/part-00000 | grep -w 'fantastic' | cut -f2,3
Q2: List of comma separated nodes the word 'fantastically' can be found in
$ hadoop fs -cat l4_output1/part-00000 | grep -w 'fantastically' | cut -f2,3,4


##### Task Two
Write a mapreduce program that processes the purchases.txt file and outputs mean of sales
for each weekday.

$ hs mapper2.py reducer2.py myinput/purchases.txt l4_output2 

Q1: What is the mean value of sales on Sunday?
$ hadoop fs -cat l4_output2/part-00000


##### Task Three: Use combiners
Write a mapreduce program that processes the purchases.txt file and outputs sum of sales
for each weekday. Make sure that the logic that you use in reducer code can be used to 
calculate intermediate value on mappers. Run the job by using combiner.

Write a new shortcut command in .bashrc
$ gedit ~/.bashrc

Write a new function called  "run_mapreduce_with_combiner". 
 
run_mapreduce_with_combinder() { 
       hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-
mr1-cdh4.1.1.jar -mapper $1 -combiner $2 -reducer $2 -file $1 -file $2 -input $3 -output $4 
}

alias hsc=run_mapreduce_with_combiner

Reload the configuration file.
$ source ~/.bashrc
 
Run Hadoop job with combiner
$ hsc ./mapper2.py ./reducer3.py myinput/purchases.txt l4_output3

