############ Task one
#
# Data description:
# It was generated by exporting data from a SQL database.
# The data in at least one of the fields (the body field) can include newline
# characters, and all the fields are enclosed in double quotes. 
# Each 'line' will be a list that contains each field in sequential order.
#
# Upzip dataset
$ tar zxvf forum_data.tar.gz
# Look at data
$ head forum_node.tsv
$ head forum_users.tsv
#
# Format of each line in forum_node.tsv is: 
# "id"	"title"	"tagnames"	"author_id"	"body"	"node_type"	"parent_id"	"abs_parent_id"	
# "added_at"	"score"	"state_string"	"last_edited_id"	"last_activity_by_id"	
# "last_activity_at"	"active_revision_id"	"extra"	"extra_ref_id"	"extra_count"	"marked"
# The above is the first line. Not every item has value. It may be "" or "\N"
# "5339"	"Whether pdf of Unit and Homework is available?"	"cs101 pdf"	"100000458"	""	
# "question"	"\N"	"\N"	"2012-02-25 08:09:06.787181+00"	"1"	""	"\N"	"100000921"	
# "2012-02-25 08:11:01.623548+00"	"6922"	"\N"	"\N"	"204"	"f"
#
# Format of each line in forum_users.tsv is: 
# "user_ptr_id"	"reputation"	"gold"	"silver"	"bronze"
#
#
# Write a MapReduce program that creates an index of all words that can be found 
# in the body of a forum post and node id they can be found in
# Do not parse the HTML. Just split the text on all whitespace as well as the following
# characters: .,!?:;"()<>[]#$=-/
#
# Test MapReduce scripts
$ head -50 forum_node.tsv | ./mapper1.py | sort | ./reducer1.py > results.txt
#
# Run Hadoop job
$ hadoop fs -put forum_node.tsv myinput/
$ hs mapper1.py reducer1.py myinput/forum_node.tsv l4_output1
#
# Q1: How many times was the word 'fantastic' used on forums?
$ hadoop fs -cat l4_output1/part-00000 | grep -w 'fantastic' | cut -f2,3
# Q2: List of comma separated nodes the word 'fantastically' can be found in
$ hadoop fs -cat l4_output1/part-00000 | grep -w 'fantastically' | cut -f2,3,4
#
#
########## Task Two
# Write a mapreduce program that processes the purchases.txt file and outputs mean of sales
# for each weekday.
#
$ hs mapper2.py reducer2.py myinput/purchases.txt l4_output2 
# Q1: What is the mean value of sales on Sunday?
$ hadoop fs -cat l4_output2/part-00000
#
######### Task Three: Use combiners
# Write a mapreduce program that processes the purchases.txt file and outputs sum of sales
# for each weekday. Make sure that the logic that you use in reducer code can be used to 
# calculate intermediate value on mappers. Run the job by using combiner.
#
# To use combiner, you will have to add a new shortcut command to your VM. 
# gedit ~/.bashrc
# In the editor that opens, find a function definition "run_mapreduce". Copy the contents 
# and create a new function (within the same file), say "run_mapreduce_with_combiner". 
# Add the following "-combiner $2" right after "-reducer $2".
# And at the end of the file, add a line for the alias:
# alias hsc=run_mapreduce_with_combiner (or whatever you called that function). 
# Now save the file and exit the gedit program. Reload the configuration file.
# source ~/.bashrc
# The new alias will take the second parameter (which is the reducer script) and also 
# use it for combiner. If you want, you can actually make another alias, that allows you 
# to use a different script for combiner. You would need to also -upload it, same as you 
# did for mapper and reducer scripts.
#
#
